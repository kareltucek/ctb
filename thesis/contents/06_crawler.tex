Consider the following problem:

\begin{problem}[ordered realization of control flow]
  Let $G(V,E)$ be a consistent flow~graph with $O$ and $I$ defined as in the \emph{vectorized code generation}. Let $G$ consist only of regular nodes (including input and output nodes), splits and merges.  Also, let control flow nodes of $G$ be expanded according to the previous section and let the factor graph of $G$ be acyclic. We wish to generate code which realizes graph $G$ on an arbitrary number of data rows, utilizing SIMD instructions whenever possible. We also wish the order of data rows ppreserved in the output.
\end{problem}

\begin{rem} 
Note that by expanding nodes we have obtained a graph which contains \emph{only} regular operations. Also, note that the graph is acyclic since we did not include any node type representing loops in the problem definition. We add loops into this algorithm later.
\end{rem}

We already know that components of $G$ can be processed as basic blocks. We propose components of $G$ to be connected by constant-sized register-mapped buffers in place of layer 1 and 2 edges. Reason for our decision to make buffers register-mapped is that we wish to minimize the amount of transfers between CPU and RAM. Reason to make our buffers constant-sized is apparently the fact that register space is limited. We again choose $w$ as the number of data rows processed in one invocation of a vectorized version of code of one partition. We also wish to preserve the order of data rows in all merges in this proposal.


To illustrate the problem, we present the following algorithm.

\begin{code}
enter the first compontent, 
  effectively pushing w data records into pipeline.
DFS( component C in factor of G )
{
  if (C has >= w records in every input)
    while (C has >= w records in every input)
      process C;
  else
    abandon current branch;
}
\end{code}

There are two problems:

\begin{itemize}
  \item If we wanted to process all data available in one go, the size of the buffers would have to increase exponentially with every merge, since every merge may have both its buffers full and thus output $2*w$ records.
  \item Since we wish merges to preserve order of all data, there may arise a situation in which a single element prevents merge. If we waited with processing of this branch until there were $w$ input data rows available, the other branch may overfill its buffers.
\end{itemize}

As a solution we propose another algorithm. This algorithm addresses the first problem by pulling new data on demand. The second problem is solved by introduction of pull semantics which cause a branch to process less than $w$ records if needed. We generate code similar to the following diagram for every component $i$.

\label{ordered_crawler}
\graphn{crawler}

\begin{description}
 \item[partition 0] For simplicity we assume that all input sources are placed in the partition 0. This is w.l.o.g since we may use a separate counter for every partition. 
  \item[terminating variable] When we exhaust all input, we set this variable to zero to indicate that the first partition should process all its remaining data and terminate. Upon termination of any component this variable is incremented and process repeated with the next partition.
  \item[remaining variable] We use this as a variable indicating count of data rows which are yet to be retrieved from all data sources. 
  \item[bold line] denotes implicit continuation, i.e., the next operation in case condition does not hold. 
  \item[solid line] denotes the next operation in case condition holds.
  \item[dotted line] denotes jumps to different partitions (in case condition holds).
  \item[rectangle] denotes a goto label.
 \item[empty inputs] When we say \emph{empty input}, we mean an input which is supposed to provide data. I.e., on split we do not jump into a branch which is not supposed to provide input.
  \item[space in buffers] When we say enough space in outputs (in context of vectorized body), we mean requirement that capacity af all output buffers of the component in question allows insertion of at least $w$ elements. When we say enough data (in context of vectorized body), we mean requirement that every input buffer if the component in question contains at least $w$ elements. When we specify input from/to \emph{j}, we mean that j denotes the partition to/from which the unsatisfied buffer leads, i.e., we are quantifying over \emph{j}.
  \item[i] An index of the current partition. These indices are topologically ordered.
  \item[j] An index of any other partition depending on context. 
  \item[handle termination zero] \ \ \ 
\begin{samepage}
\begin{code}
if( i == 0 && remaining < w )                            
{                                                          
  terminating = 0;                                     
  if(remaining == 0)                                   
  {                                                    
    terminating = 1;                                   
    goto PARTITION_1;                            
  }                                                    
  goto SINGULAR_BODY_0;
}                                                      
\end{code}
\end{samepage}
\item[handle termination nonzero] \ \ \ 
\begin{samepage}
\begin{code}
if ( i == terminating )                                   
{                                                          
  if(an input of i is exhausted)                  
  {                                                        
    terminating++;                                         
    goto PARTITION_i+1;                                
  }                                                        
  goto SINGULAR_PARTITION_i;
}   
\end{code}
\end{samepage}
\end{description}

\begin{claim}
  We claim that the proposed solution solves th problem \ref{TODO} under the stated conditions if all buffers have positive capacity. That is:
\begin{enumerate}
  \item Algorithm produces correct results (if it terminates).
  \item Algorithm does not enter an infinite loop during the pre-termination phase unless there is an infinite loop contained in the semantics of $G$ with respect to the data processed.
  \item Algorithm terminates correctly.
\end{enumerate}
\begin{proof}[Proof of 1]
  Our algorithm never retrieves data from an empty buffer and never pushes data into a full buffer. Also we required $G$ to be consistent with the semantics of the defined control flow node types.  
  Consistency of $G$ ensures that all data of any single data row will get processed without leaving unprocessable elements in buffers. This means that as long as elements of all data rows pass through all operations in order, there is no way how could two elements from different data rows get processed as if they were from the same data row.  Thus all results are correct. 
\end{proof}
\begin{proof}[Proof of 2] So let there be a flow graph $G(V,E)$ and a sequence of data rows such that the described algorithm enters an infinite loop which does not process any data. Let $C$ be this loop (subgraph of factor graph $H$ of $G$)\footnote{This is a directed acyclic graph with all degrees equal to two, not a directed cycle!}. Let $t$ be a topological ordering of $H$. Also let there be a description of content of buffers at the moment when algorithm iterates over $C$.
  \begin{itemize}
    \item The asumption that $C \in H$ is correct since if there is an infinite cycle which uses an edge on layer 2 then the algorithm processes data and hence, behaves correctly. Besides, we have not allowed any cycles (and thus edges on layer 2) to exist in $G$ (yet).
    \item Apparently the algorithm iterates only over singular code of partitions. This means that all partitions in $C$ have either entirely full output or entirely empty input. Also no partition is contained in $C$ more than once.
    \item Let $M = \{v \in C \mid \neg (\exists u \in C)( u <_t v \land (u,v) \in C )\}$ (informally all minimal vertices in $C$ with respect to topological order of components restricted to edges in $C$). Let $n$ be the largest index of a data row such that serie $n$ was processed by all components. Such $n$ exists since every components in $M$ has a full output queue (otherwise our algorithm would dot continue downwards).

    \item Let $a \in C$ be any partition which the $n$th data row is the last processed one. Then there is an output buffer of $a$ which contains data from the $n$th serie. Let $e \in C$ be an edge which corresponds to this buffer.  Let $b,c \in C$ such that there exists paths $P_{ab}, P_{cb} \in C \land e \in P_{ab} \land c \in M$. 

    \item Let $m$ be the highest index such that an element from $m$th serie is still present on path $P_{ab}$. 

    \item Partitions $a,c$ have processed the $m$th serie since $a,c \in M$ and $m \geq n$. Partition $b$ has not processed the $m$th serie since an element from the $m$th serie is still on the path $P_{ab}$. That means that there is an element from the $m$th serie on path $P_{cb}$. (This holds since our algorithm never chooses a branch in which no data from the required data row are to be found, and since the required data row is exactly the $m$th one. That is also the reason why we did not use the $n$th serie.) But that is a contradiction because one of the two paths is empty.

  \end{itemize}
  \graph{cycle}

\end{proof}
\begin{proof}[Proof of 3]
It suffices to check that the algorithm will process all data of the terminating component and increment the \emph{terminating} variable.
\end{proof}
\end{claim}

Thus, we have shown that general consistent flow graphs consisting of regular nodes, merges and splits may be realized by this approach.

\subsubsection{Loops}

It is easy to see that the proposed \emph{LOOP MERGE} operation is well defined and that it behaves as expected in the definitorical sense given in \emph{realization of a flow graph on a single data row} (define \ref{TODO}). It may seem, that the previous algorithm still provides correct results on a graph with \emph{LOOP MERGE} nodes, since the second point of the proof still holds. The real problem comes with data ordering as the following ideas illustrate:
\begin{enumerate}
    \item Moving data over a backward edge may result in insertion of these data after data from a data row with higher index. 
    \item The point 1) does interfere with the first point of the proof badly since the merge operation is not deterministic. 
    \item We may decide to let multiple data rows enter the loop and then postpone exit of every row with non-minimal index (with respect to being in the loop in question). We would then pull the minimal-index row through the loop using the nonvectorized pull-semantics\footnote{By pull semantics we mean the mechanism which allows processal of data in nonvectorized manner} until it exitted the loop. This does not work due to point 1). We would have to implement multiple `waiting' buffers at every \emph{LOOP MERGE} and we would still have to pull a significat amount of data by the nonvectorized bodies. Actually the efficiency of this approach would decrease with increasing $w$ due to increasing probability that a non-minimal row will need to exit the loop prior to the minimal row.
    \item We may improve the semantics of the \emph{LOOP MERGE} node so that elements of data rows remain synchronized inside of loops and then add a `waiting' buffer at the exit from the loop which would sort the rows into the initial ordering. This does not work since the row waited for may remain in the loop for a very long time. (We would need to use unbounded queues instead of constant-size buffers)
    \item We may employ the previous point other way around. We may make all other data wait for data coming from loops. This approach does work except for the problem that the rest of the network is performed in noninitial order. This is approximately the way the second proposal copes with the problem. We cannot use it here since we required data to be processed and outputted in-order. For this reason there is no algorithm which processes arbitrary loops and outputs data in-order and which does not use unbounded queues.
    \item We may ensure that at most one data row enters the loop at the same time. This makes sense since the theoretical single data row apparently works. 
\end{enumerate}

The approach suggested by 6) and 3) (which is basicaly more complicated version of 6) ) may be realized and would work in a \emph{single row realization} environment on general flow graphs consisting of the node types defined. We provide an example which shows that the algorithm \ref{TODO} does not work with flow graphs with generally placed \emph{LOOP MERGE} nodes. Code generated by the algorithm \ref{TODO} for the following example may change order of some data rows despite consistency of the flow graph. 

\graph{counterexample}

Restricting the algorithm \ref{TODO} with allowed \emph{LOOP MERGE} nodes to flow graphs generated by the means of observations \ref{TODO} - \ref{TODO} solves the problem.

\begin{define}[implementation of LOOP MERGE operation]
  Let every LOOP MERGE operation have a boolean flag as its state, initialized to false. Modify the algorithm \ref{TODO} to set this flag to $true$ every time any partition processes data. Furthermore, modify the algorithm \ref{TODO} to use only singular body of the LOOP MERGE node (this is no problem since LOOP MERGE node is always the only one in a partition). Let the code of the partition of LOOP MERGE node be defined by the following code.
\end{define}
\begin{code}
SINGULAR_PARTITION_i:
PARTITION_i:
if(state)
{
  state = false;
  //k denotes partition of the second input
  goto SINGULAR_PARTITION_k; 
}
if(output to l is full)
  goto SINGULAR_PARTITION_j;
if(second input is nonempty)
{
  process the second input;
}
else
{
  if(input from j is empty)
  {
    if(terminating == i) 
    {
      terminating ++;
      goto SINGULAR_PARTITION_i+1;
    }
    //j denotes partition of the first input
    goto SINGULAR_PARTITION_j;
  }
  process the first input;
}
repeat
\end{code}

\begin{claim}
  We claim that the algorithm \ref{TODO} implementing \emph{LOOP MERGE} nodes as in the definition \ref{TODO} solves the problem \ref{TODO} with \emph{LOOP MERGE} nodes if $G$ was created by the means of composition of schemas \ref{TODO} and \ref{TODO} by the means of observations \ref{TODO}-\ref{TODO}.
  \begin{proof}
    We need to check that the three points of the proof \ref{TODO} still hold.
    \begin{enumerate}
      \item The state ensures that as long as there is a data row present in the loop, new row is not retrieved from the first input. Data obtained from the second input must have been earlier processed by the first input. Thus data remain in order and valid.
      \item Note that if no data were processed second time a \emph{LOOP MERGE} partition is entered it continues to the partition of the first input. This means that either:
       \begin{itemize}
         \item Some data were processed. In this case a data-processing infinite loop is correct behaviour.
         \item No data were processed. In this case the cycle does not use the second input. Thus, the original proof holds.
       \end{itemize}
     \item This partition apparently terminates correctly (unless there is an infinite loop in semantics of $O$ with respect to the data being processed).
    \end{enumerate}
  \end{proof}
\end{claim}


\subsubsection{Analysis}
We provide some observations relating to efficiency of the provided solution.

\begin{observation}
  Let there be two parallel branches of computation leading from a common predecessor partition to a common sucessor such that both branches receive data from the same data rows. Let the sum of buffer capacities of one of the branches be significantly higher than the corresponding sum on the other branch. This means that one branch has significantly higher average density of data per buffer. Notice that now the output buffer of the branch with smaller capacity is likely to cause the content of the other branch to be processed even though most of the buffers on this branch does not provide enough data for vectorized processal.
\end{observation}

We believe that the reader has some intuitive understanding of this observation. We do not provide exact definitions since we do not draw any exact conclusions and since these definitions would be truly tedious.


We propose assigning buffer sizes according to the following algorithm although we have no proof of feasibility which would have realistic asumptions about the input data. Let $H$ be an acyclic factor graph of a flow graph $G$ with expanded control flow nodes. Let $w$ be (again) a chosen size of vectors and let $c \in \N$ be an arbitrary coefficient.

\begin{samepage}
\begin{code}
fun l(partition d) { an integer anotation of components of H}

fun balance_sizes(H, w, c)
{
  foreach(d in H)
    l(d) = 0;
  while( exists edge (a,b) in H s.t. l(a) + 1 > l(b))
    l(b) = l(a)+1;
  foreach(buffer (from a to b) in H)
  {
    size(buffer) = (l(b) - l(a))*w*c;
  }
}
\end{code}
\end{samepage}

\begin{observation}
  Suppose that all output buffers of topologically minimal and all input buffers of topologically maximal partitions (in factor graph of $G$ w.r.t. connectedness) have size at least $w$. We claim that if we uncomment the commented node, i.e., if we force vectorized processing of those partitions, then the algorithm will still work.
  \begin{proof}
    Vectorized and singular processing are equivalent in these partitions as long as buffers have sufficient capacity. This holds since these partitions will never be required to forward data from a predecessor to a succesor. 
  \end{proof}
\end{observation}

\begin{rem} 
  Although this observation may not seem interesting for theoretical analysis, it is vital for efficient implementation of vectorized load and store operations. (This is due to typical alignment requirements of fast load and store operations)
\end{rem}

