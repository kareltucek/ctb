Sat May  2 11:42:39 CEST 2015
=============================
The basic idea is the following:

Backend will serve as a library frontend. It will manage an instruction table, loading and processing of all data. It will be parametrized by a type of the instruction table and loader. Backend will forward arguments to the graph and instruction table functions.

Loader will represent a single consistent input method. Loaders may be swapped over the backend. E.g. one loader may handle xml files while another plain text files. Loader will construct the instruction table and graph using their public methods. Instruction table may be replaced by a self-contained instruction table - in that case backend should be parametrized by a loader with an empty call to the method loading the instab.

Writer represents a block of code and provides methods for simple generation and formating of code. It is basically a bit tweaked string.

Instruction table will hold the code generation information.

Graph will hold the graph with appropriate links to the instruction table. Graph may be split into Graph and Generator classes in future.

Code for generation will be held as part of xml files and will use a shell like evaluation of parameters using printf like syntax with predefined meaning of arguments.

Sat May  9 12:40:39 CEST 2015
=============================
Today I've found a problem of the writer concept. The problem is that a simple writer can work only with one abstract level of aliases, which means that deeper evaluation has to be known to all levels (e.g. variable names and indexes have tobe distributed into the instruction table xml description, which though should describe instructions without carrying the resulting code as its own dependency). As an answer to this problem I decide to introduce a concept of a model. Model is a static extension to the writer class, representing a consistent output system. Exampli gratia we may have one model representing a bobox box or project or a testing (nonbobox) suit dedicated to the code generation. Model may have multiple levels and should be able to evaluate abstract aliases on multiple levels. For instance one level may carry a structure of file templates and provide a mechanism of putting these together. Another level may translate abstract instruction table arguments into actual variable names with corresponding indices. 

Proposed implementation:
- Make all generating graph funcions templated, taking an instance of a mode/writer, which it will then use for output.
- Generalize alias evaluation by adding a static map to a model. Lets consider print("$output = $arg1", value). Model will have defined an alias of an $output to "$outname$outindex[j]", $outname to "out_list_", $outindex to "$3" and $arg1 to "$4". This will be evaluated by a recursive sequence of print calls out of which all will carry the same parameter list, so that the leaf arguments $3,$4 etc will be evaluated according to the caller's context while its abstract meaning will be given by the model.
- Model will provide a main method, which will generate the resulting code. This method will be called by backend's process and will take a graph as its argument. The main method will then write its code and call graph's generate method parametrized by itself, so that the code generation made by graph will get evaluated by the alias definitions of the model itself.


Mon May 11 20:33:37 CEST 2015
=============================
Today I have refactored the writer class. Now it resolves aliases in arguments as well (a real variadic list hell). Furthermore I started implementing the SSE instructions. 

I had sorted out type conversions. The data producer is always responsible for providing the data in all required widths. The output width is the main width specifier for a regular node.

All instructions produced by one node are packed in the final ordering. Whe could change the ordering to produce less spill code by moving the induction variable above the tree crawler function -> then every node would have to remember the actual number of iterations produced and the crawler would have to respect these, possibly returning to any node multiple times, going through segments between merges. Compiler should be able to take care of this automatically.

I consider adding a 'custom code' field into the xml structure to allow user to define his own call structure e.g. when he needs any temporary variables. There have already been problems with conversions, because it may not be possible to implement split instruction in a generic way (i.e. by a common template for both halves which are produced).

Also I considered the control flow version of this problem. The proposed solution is to create one more graph, which will hold all components determined by conditional cuts. This will hold the code and various cut flags for every one of these. The final code will be produced by DFS on this graph. The generic part of output will have to be generalized using more templates somehow in order to take care of ingeneral control flow structures. 


Tue May 12 16:45:32 CEST 2015
=============================
New problem has occured with the right hand dollar sign. It will have to be resolved by a manual flag (we are out of control of how many times it may be processed and thus we dont shorten the $$ to $ until final output unless explicitly asked for. The problem occured when constructing intermediate names in the split/merge functionality.


Thu May 14 12:30:39 CEST 2015
=============================
Today I have split the original grap into a generator and a graph class. Also I have created a proxy class wrapper, which allows read-only access to foreign classes and write access to owners of an object. I failed to do this transparently (as I originally itended).

Wed Jun  3 10:02:58 CEST 2015
=============================
In the past two weeks I implemented a conversion path search and wrote some documentation.

Fri Jun 26 11:30:16 CEST 2015
=============================
Today I am considering how to actually import data. Some type abstraction seems to be necessary (e.g. to import all instructions of the same format (e.g. add_pi[u]{32|60|128}($arg1, $arg2)) by only one line. For this purpose csv format seems to be much more versatile, since it allows easy 'foreach' expansions. As the result I am implementing a csv importer with a perl expansion script.

Also today I noticed a few problems with the writer design - mainly ingenerality. At the moment I would like to be able to use it simply with multiple output formats, but I have hardcoded syntax structure. 

Next thing I ran into is a serious problem with loading system, which does not allow combining multiple loading systems. Fortunately fix seems to be simple.


Sat Jul 25 10:56:04 CEST 2015
=============================
Yesterday I finished the csv preprocessor discussed in the pevious entry.

Now I have arrived at another design problem. Instruction table does not 


Thu Jul 30 10:01:27 CEST 2015
=============================
I am implementing a csv loader and exporter atm. Due to the need of exporting data most of members of instruction_table have to be made publicly readable. 

The next thing to be done is generalization of the command line interface. The current one is totaly ingeneral. For this, the model/loader management will have to be improved. Establishing functional hash tables holding template-constructed functors seems to be a good idea for this.


Sun Aug  2 19:49:21 CEST 2015
=============================
Note that model/loader template signatures have to take form of "typename...". All other parameters have to be packed.


Tue Aug 11 14:19:34 CEST 2015
=============================
Back again. Today I started testing of the csv loader and bumped into problems with ingenerality of the writer. The strings to be imported/exported tended to be expanded when they should not have been. As a result I had to implement explicit 'dolar_mode' control for both input and output actions of writer. 

Another thing I have done is generalizing exception handling. I have created two global functions which would let me change mechanics of exception throwing simply and also have written multiple layers of error handling.


Wed Aug 12 19:19:15 CEST 2015
=============================
I restructured the directory structure.

Exception handling was improved in order to report helpfully errors in (csv) instruction tables.

Also I have created basic C instruction table.

Now I am working on tags and custom code support, because I need to type in tags while creating the sse instruction set.

I am introducing new structure: a tagmaster. Motivation is described in tagmaster.h.

TODO
====
// TODO
//conversions - find nontrivial conversion path -implemented
//output node ordering - dropped
//abstract output syntax formatting - done
//export mechanics - hopefully done
control flow support

// TODO - minors
//stream support - hopefully done
//ensure correct clearing - done
//allow custom combination of loaders - done
general conversion interface
tag support
custom code support
switch to nonstatic loaders
unit tests for export mechanics (and dependencies)
split commandline interface from programming interface (taskmaster conflicts at the moment)


